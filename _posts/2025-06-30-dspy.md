---
layout: post
title: "AI programs"
description: "DSPy"
date: "June 30, 2025"
code: true
---

What if you didn’t have to be dependent on strategies, optimizations, and models to improve AI task performance?

No more watering the garden squares of “do this not that” edge cases. No more “prompt engineering”.

Just program logic and optimization metrics.

Enter: [DSPy](https://dspy.ai/)

## Probability example

### Before DSPy

```python
prompt = "You are a helpful assistant. Answer this math question step by step: Two dice are tossed. What is the probability that the sum equals two?"

response = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)
# Yes a dumb case. But string-based prompting performance is brittle.
```

### After DSPy

```python
# Declarative, structured approach
import dspy

# Configure your language model once
dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

# Define behavior through signatures, not strings
math = dspy.ChainOfThought("question -> answer: float")

# Use the module - DSPy handles prompting automatically
result = math(question="Two dice are tossed. What is the probability that the sum equals two?")

# Get structured output with reasoning
print(result.reasoning)  # Step-by-step explanation
print(result.answer)     # 0.0277776
```

Okay, this example didn’t mean much to me when I first saw it. Let’s see a more real-world use case: extracting information from emails.

## A Customer Support Email Classifier

### Before DSPy

```python
def process_email_old_way(subject, body, sender):
    # Separate prompts for each task - brittle and hard to maintain

    # Email classification
    classify_prompt = f"""
    Classify this email as one of: order_confirmation, support_request, meeting_invitation, newsletter, promotional, invoice, shipping_notification, other

    Subject: {subject}
    Body: {body}
    Sender: {sender}

    Classification:"""

    classification = call_openai(classify_prompt)

    # Entity extraction - different prompt structure
    extract_prompt = f"""
    Extract the following from this email:
    - Financial amounts (format: $X.XX)
    - Important dates (format: MM/DD/YYYY)
    - Contact information
    - Action items

    Email: {subject} {body}

    Extracted info:"""

    entities = call_openai(extract_prompt)

    # Urgency detection - yet another prompt
    urgency_prompt = f"""
    Rate the urgency of this email from 1-4:
    1=low, 2=medium, 3=high, 4=critical

    Consider: {subject}

    Urgency level:"""

    urgency = call_openai(urgency_prompt)

    # Manual parsing hell
    try:
        # Hope the LLM returned exactly what we expected...
        classification = classification.strip().lower()
        urgency_num = int(urgency.strip())

        # Parse entities with regex and prayer
        amounts = re.findall(r'\$[\d,]+\.?\d*', entities)
        dates = re.findall(r'\d{1,2}/\d{1,2}/\d{4}', entities)

        return {
            'type': classification,
            'urgency': urgency_num,
            'amounts': amounts,
            'dates': dates
        }
    except:
        # When it inevitably breaks...
        return {'error': 'Parsing failed'}

# Problems:
# - 4 separate API calls (slow, expensive)
# - Fragile string parsing
# - No consistency between outputs
# - Breaks when switching models
# - Manual prompt engineering for each task
# - No systematic way to improve accuracy
```

Want to optimize?

```python
# When accuracy is poor, you manually add examples:
classify_prompt = f"""
Examples:
"Server down" -> support_request, critical
"Order confirmed" -> order_confirmation, low
"Meeting tomorrow" -> meeting_invitation, medium

Now classify: {subject}
"""
# Still brittle, still manual...
```

### After DSPy

```python
import dspy

class EmailProcessor(dspy.Module):
    def __init__(self):
        # Define WHAT you want, not HOW to prompt for it
        self.classifier = dspy.ChainOfThought(ClassifyEmail)
        self.entity_extractor = dspy.ChainOfThought(ExtractEntities)
        self.action_generator = dspy.ChainOfThought(GenerateActionItems)
        self.summarizer = dspy.ChainOfThought(SummarizeEmail)

    def forward(self, email_subject, email_body, sender):
        # Compose modules together - DSPy handles the prompting
        classification = self.classifier(
            email_subject=email_subject,
            email_body=email_body,
            sender=sender
        )

        entities = self.entity_extractor(
            email_content=f"{email_subject}\n{email_body}",
            email_type=classification.email_type
        )

        # Get structured, typed outputs automatically
        return dspy.Prediction(
            email_type=classification.email_type,
            urgency=classification.urgency,
            financial_amount=entities.financial_amount,  # Proper float
            important_dates=entities.important_dates,    # Proper list
            action_required=True if classification.urgency == "critical" else False
        )

# Usage - clean and simple
processor = EmailProcessor()
result = processor(
    "URGENT: Server Down",
    "Production is offline, need immediate help",
    "alerts@company.com"
)

print(result.email_type)      # EmailType.SUPPORT_REQUEST
print(result.urgency)         # UrgencyLevel.CRITICAL
print(result.financial_amount) # None (properly typed)
```

Want to optimize?

```python
# Load your email dataset
emails = load_historical_emails()  # 1000 labeled emails

# Define success metric
def email_accuracy(example, prediction):
    return (example.email_type == prediction.email_type and
            example.urgency == prediction.urgency)

# Optimize the ENTIRE pipeline automatically
optimizer = dspy.MIPROv2(metric=email_accuracy)
optimized_processor = optimizer.compile(processor, trainset=emails)

# Optimized prompts for each module
# Handles edge cases automatically
```

You should probably be using DSPy.

## Resources

Check out my [AI tools & resources reference](https://webref.lukasmurdock.com/ai)
