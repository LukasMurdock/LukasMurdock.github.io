---
layout: post
title: "Local LLM"
date: "July 08, 2025"
code: true
---

macOS, [Ollama](https://ollama.com/) makes it easy to run LLMs locally.

As of writing, small LLMs capable of running on 16GB of ram have limited coding ability - when compared to new models from Anthropic they are not even worth it. However, as a form of text compression, small LLMs are very useful to have - and have the added benefit of working offline.

Download Ollama

```shell
brew install ollama
```

Start Ollama server

```shell
ollama serve
```

Download a model

```shell
ollama pull llama3.2:3b
```

Run a model in the command line

```shell
ollama run llama3.2:3b
```

## Opencode

For actual use, you will want to change the prompt default limits of Ollama by using a Modelfile.

Example modelfile (this can be named anything)

```
FROM llama3.2:3b

# Increase context window for larger codebases (32K tokens)
PARAMETER num_ctx 32768

# Lower temperature for more focused, deterministic generation
PARAMETER temperature 0.1

# Reduce repetition
PARAMETER repeat_penalty 1.1

# Look back further to avoid repetitive patterns
PARAMETER repeat_last_n 128

# More conservative sampling
PARAMETER top_p 0.9
PARAMETER top_k 40

# System message
SYSTEM """Insert your system prompt here"""
```

```shell
ollama create <new_model_name> -f <path_to_model_file>
```

Connect a model to [opencode](https://github.com/sst/opencode)

```json
{
	"$schema": "https://opencode.ai/config.json",
	"provider": {
		"ollama": {
			"npm": "@ai-sdk/openai-compatible",
			"options": {
				"baseURL": "http://localhost:11434/v1"
			},
			"models": {
				"llama3.2": {}
			}
		}
	}
}
```

## Making the most out of small models

Your prompt > LLM prompt optimization > LLM search engine query results > LLM summarization / tool calls.

See [How to fix your context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html).

### Prompts

- [opencode](https://github.com/sst/opencode/tree/dev/packages/opencode/src/session/prompt) prompts
- [fabric](https://github.com/danielmiessler/fabric/tree/main/data/patterns)

### Strategies

- [fabric](https://github.com/danielmiessler/fabric/tree/main/data/strategies)
- [DSPy](https://dspy.ai/api/modules/ChainOfThought/)

Claude models have "think" directives".
